{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install 🤗 Transformers and 🤗 Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsHUjgdIrIW",
        "outputId": "018d775a-2d5d-4fc8-9e4b-442b63677561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoQGscDt1fKS"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
        "\n",
        "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "664d7c2c34c344c4813f9a3a7e9526f0",
            "00858ad831ad45c9b810b95c22a0b3b8",
            "e156e839e650459ca209b985b1a01914",
            "cd1e3f1581e8431a9710cf90d8b24388",
            "72af77165851496fb0db8d2676974d59",
            "7fd3275f9caa40a8a860a0763c73aa64",
            "6f2b6d1924164d6d8c4a4ecb5f959057",
            "362390504df94a7ab41aa1093b65a482",
            "e1f79ff76dcc4bcfa92b2347b1df0af7",
            "34d6c317a04d4139a3dcae157dae828f",
            "5a7785a920f14b2a84f3e8e917f02c12",
            "6042203aa65f4456a45732ea7f737d7c",
            "211e7a4f3a554d8495bc064d9d708a07",
            "d92c2d7c10b24fcd95e1cec9ebcea0e5",
            "403c4b7a80c5451597814592797fa7b7",
            "dc2d92ec876e45bfaac882031e580161",
            "324cfb7794ec4e42902a32470ffe456a"
          ]
        },
        "id": "X6XxX9ya1fKU",
        "outputId": "1126f81a-ef45-4c18-a3c4-eb2151895c69"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "664d7c2c34c344c4813f9a3a7e9526f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0Dhh9521fKV"
      },
      "source": [
        "Then you need to install Git-LFS. Uncomment the following instructions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBQQpM-y1fKW"
      },
      "outputs": [],
      "source": [
        "# !apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjD9jfGQ1fKX"
      },
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cll1Kjgt1fKX",
        "outputId": "8bfd3e46-3309-447a-a226-6f877457a02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.18.0\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/text-classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a text classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model to a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/).\n",
        "\n",
        "![Widget inference on a text classification task](https://github.com/huggingface/notebooks/blob/master/examples/images/text_classification.png?raw=1)\n",
        "\n",
        "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
        "\n",
        "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a  dataset containing sentences labeled grammatically correct or not.\n",
        "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
        "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
        "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
        "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
        "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
        "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
        "- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
        "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)\n",
        "\n",
        "We will see how to easily load the dataset for each one of those tasks and use the `Trainer` API to fine-tune a model on it. Each task is named by its acronym, with `mnli-mm` standing for the mismatched version of MNLI (so same training set as `mnli` but different validation and test sets):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZbiBDuGIrId"
      },
      "outputs": [],
      "source": [
        "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a classification head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "task = \"eoir_privacy\"\n",
        "model_checkpoint = \"allenai/longformer-base-4096\"\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "Apart from `mnli-mm` being a special code, we can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "64e2250b867944bd87e82bc9b7311ed0",
            "3da93abdc68c42e0accdb025f774aa1a",
            "cf300dc673c744d9957a00c7167aafee",
            "4b2528fea8054ddc88c13f0db936dc7b",
            "282ec4abcc39445ba42e4bbc58cff5c3",
            "4bdb57458ae346399b52a5d71aea0df6",
            "668c82df4a0e4d5ab275942e9dc033f2",
            "75b9061d9ef24b56915b44230eea44c0",
            "aa1f9f2ea87b4ad5a3592f18b62da0d2",
            "4b313a595e5545f092ff5b8634166e14",
            "2c6f1984fc594dbfb2f9dd6a7642b13a",
            "ceb6b3e11ea8473792231297e924c16a",
            "08cc930333754a7d9a2d2d8bf301b8cf",
            "2c387ea80fe643d48c4dfdfd04eb160e",
            "9f490d02ca274614b5cf3a2ce670a1db",
            "dbd93cf11f4643c89ca14ace4bf93498",
            "417d3e7c4d974a31b33a4a74ab2532e9",
            "f702d59ce3af429fbb464b9d768a1d14",
            "9b56bb14b8d741a9ace5579f71408b82",
            "c6be8ecc7b024b97937ed691b2235dc8",
            "2ff528c8f7e54e8298ed407defdcff3b",
            "9d9f23b42a6548758b81147ac31bf5a2",
            "87677cac852240a8ae27b4171c9301a6",
            "f3aa20e336fb4a5290dc6e424c1fdbb4",
            "578e7e15451f41969e0f30a822c1ae96",
            "a4a07ee286414788a77beb3ee3fbb2cc",
            "476e4f3647a045c5a014d345ce91865b",
            "b731387ad93c499691167c4b77ae2811",
            "4340593483c14282841abd25fee8f86a",
            "372b611c73a44c36b64df1922b108c02",
            "b77ad20018524ec69241a6100cde18ef",
            "48180c0d6f8645b087ca99f1583e4e2f",
            "a9506157b54b4114a5b622889b4ffadc",
            "23cac59fa57940aa8f63ab564136f7a4",
            "e6943ed1569b46f498af50c7cf195f44",
            "658cff5ea4eb42078963d2a227a4e647",
            "a18390342e47484c946fabf46e716515",
            "c975b61136c94b67a0e857e80f056d7b",
            "7f7a6a4d75df4d08af50bac590c84a59",
            "b7f6c5d7bc7f4df8a40ce983dd7c65e5",
            "772fb7fac1744e65b4ec4e6a3f86e043",
            "186aa4cf86a34f4b9ba1e829778ee58e",
            "7ddb172f0e954414a317a16a1d6fe31c",
            "58b2e4bea7a64c3bbde79a2d5d43f7d0",
            "930769b32169425bbb26f61f17440d51",
            "8d352e05994f408abbfb6bc12afd3764",
            "88b011ba7165406d8c00a971011b739c",
            "c5f078557e214d599c5c74c9694957b3",
            "c69ae8ba2be14644ba97e519100bb372",
            "c65d50e153a541ac974ff2c2445ac78c",
            "f1aff7ea074948a1a5256c33bfb632ff",
            "85e0a2a479bf4b5f9a6cfa99685bb7ab",
            "6dac1446a8c04e3c9393cefc98e32b00",
            "dd9cbf26f7bf484795f8193a62098c69",
            "ae43fdaf988e4f1ba7a72acbfd198de3",
            "ce27eb1a6a074360adeeaa42c0351c71",
            "85dbdb4fb3cc48659c37976db5b6ac78",
            "75556157e2074b9aa90606ea9e9da618",
            "1c4768f80a594073a0c5db872c1a17f3",
            "cf555a4443d34486abae8560abd9f823",
            "66724752df9944ae8d04377e162e9a88",
            "7ab855ab579d42d285c2cfa58796ba21",
            "c48177d5b3d04817ba71fc3f12450616",
            "4140bd879ef64c378e723f515e1dcc7e",
            "3a01301bfa834fe9a3ed0e970b135dec",
            "c825a55402574fada73d3c870ea6536f",
            "f8af2e05a1654404b7f8648cb37f0ab2",
            "2b56e454602941dda8fc6622eac051b9",
            "76a025fdfd5e44ec8c73ca19f1d2f47d",
            "98a00fe3243a4a5ab572634ddec68a38",
            "5360d869ee2342cbaad6d4df3f7dc576",
            "d1d3d1ee4d20407fbd6c29c7bc1b4bb3",
            "31222f685f844ed488e40b24b071e9fe",
            "bbcd6f92c2884b4284177b3051e6280e",
            "b96a1e56b1954b96a52baa5d91b801f1",
            "531d9e4d265841839336711a5c4a1eeb",
            "6508db387816439cb41ddc704b879249",
            "113ce950efcd473e869797346ab74b36",
            "2cf7cb7c544a459797406d42cc219cec",
            "c4651d73a7f54ab284c6392744eded7f",
            "5e22620dcdef4e62b71f08e711eb78db",
            "d505bd73227c49668c298a2d2bcbc6c0",
            "14dd88362bf84377938b5d3b47a48f9c",
            "f61338d982704978bea80ff3fe2aadd7",
            "db9746194f25428d92ccfefaf972b01a",
            "f3024bfb689240828653eba11bbb70d3",
            "eb83a716f28a469b89cb85011327ef9d",
            "f22cac22604f441ca818c3c3aa42aa42"
          ]
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "63775619-e211-4456-da16-a65dd31ef1ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64e2250b867944bd87e82bc9b7311ed0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.76k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset eoir_privacy/all (download: 2.31 MiB, generated: 11.46 MiB, post-processed: Unknown size, total: 13.76 MiB) to /root/.cache/huggingface/datasets/pile-of-law___eoir_privacy/all/0.0.0/0bc7ce03d7dcc8a12e99cb3959976833c05daf1da2e98a7613e734e934fed111...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceb6b3e11ea8473792231297e924c16a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87677cac852240a8ae27b4171c9301a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.89M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23cac59fa57940aa8f63ab564136f7a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "930769b32169425bbb26f61f17440d51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/528k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce27eb1a6a074360adeeaa42c0351c71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/6309 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8af2e05a1654404b7f8648cb37f0ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/1552 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset eoir_privacy downloaded and prepared to /root/.cache/huggingface/datasets/pile-of-law___eoir_privacy/all/0.0.0/0bc7ce03d7dcc8a12e99cb3959976833c05daf1da2e98a7613e734e934fed111. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "113ce950efcd473e869797346ab74b36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"pile-of-law/eoir_privacy\", \"all\", use_auth_token=True, download_mode=\"force_redownload\")\n",
        "metric = load_metric('glue', \"mrpc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWiVUF0jIrIv",
        "outputId": "9a6aed77-ad1e-4f90-9ad4-4f00f9c19aec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 6309\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1552\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6HrpprwIrIz",
        "outputId": "16b375e3-dddc-4eff-92db-e5ddf1f28c67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'label': 1,\n",
              " 'text': \"\\x0cheld that a prosecution for violation of a city ordinance, while in the form of a criminal prosecution, is, in fact, a civil proceeding to recover a penalty and a preponderance of the evidence is all that is required to sustain a conviction.' He concludes that the criminal . section 241(a) (4) is not sustained as a matter charge laid unticr of law. We do not concur. Here we are concerned with interpreting a Federal statute in which Congress has expressed its disapproval of the type of behavior for which [MASK] was convicted under a city ordinance in the State of Nebraska. The fact that the misconduct is considered a civil proceeding by the courts of that state does not control when interpreting the immigration laws. Au act of Congress is not circumscribed by restrictive holdings of state courts defining the jurisdictional and procedural limits of inferior courts of criminal jurisdiction.\"}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SZy5tRB_IrI7",
        "outputId": "2a031955-d1e8-42a4-b946-85dde2171b33"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the final conclusion was that he not only failed to sustain the burden of proving good moral character for ten years, but that the evidence conclusively showed that he was not a person of good moral character during that period. There is nothing in the Orlando opinion which would indicate that the Government urged that 8 U.S.C. 1101(f) (6) precluded a finding of good moral character in his case. There is a statement on page 851 of the court's opinion reading, \"* * * Orlando argues that [MASK] has to be a special kind of a prevaricator in order to be ineligible for suspension of deportation as defined by 8 U.S.C.A. § 1101(f) * *.\" We assume that counsel for Orlando was urging that he did not come within any of the eight numbered paragraphs of S U.S.C. 1101(f) and that, therefore, a finding of good moral character should be made. That argument, of course, ignores the last sentence of the statutory provision which provides otherwise. It appears to have been due only to this contention of counsel that 8 U.S.C. 1101(f) (6) was considered by the court; although, as we have indicated above, it was actually inapplicable to Orlando's case. Since the question involved in this [MASK]'s case is whether a false statement in an application is false testim-ony within the meaning of 8 U.S.C. 1101(f) (6) and since that question was not even discussed in °Timid o, that case has no relevance to that of this [MASK]. With respect to Sh,tivih-a v. Hoy. supra, the special inquiry officer had held there that the alien was ineligible for voluntary departure on the ground that he had given false testimony for the purpose of obtaining a benefit under the immigration and Nationality Act. On August 22, 106, lie had executed under oath an application for 401</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>States citizenship. The record shows that [MASK] at least suspected that he had United States citizenship since the age of ten, and that he clearly knew of his status after his relatives visited the consular official in 1957. The cases relied on by counsel therefore are not in point, and we need not now consider the questiom that would be presented if [MASK] had at all relevant times been ignorant of his claim to United States citizenship. Cf. Petition of Acchione, 213 F.2d 845 (C.A. 3, 1954). Counsel recognizes that the Supreme Court upheld the constitutionality of section 301(b) in Rogers v. Bellei, 401 U.S. 815 (1971). Counsel, however, attempts to distinguish Bellei on the ground that the individual involved in that case was clearly aware of both his citizenship and the retention requirements. Counsel evidently contends that ignorance of the retention requirements alone is significant because of the Supreme Court's decision in Schneider v. Rusk, 377 U.S. 163 (1964). In Schneider v. Rusk, supra, the Court declared unconstitutional the expatriation of naturalized citizens merely for residing for several years</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This matter has been certified to the Regional Commissioner for review of the decision of the District Director denying the application for adjustment of status. [MASK] is a 29-year-old married male native and citizen of Colombia who last entered the 'United States at Miami, Florida on October 15, 1961 as a nonimmigrant visitor for pleasure. On November 1, 1961 he submitted the instant application for status as a permanent resident under section 245 of the Immigration and Nationality Act. The application reflects that the alien has been married only once and that he has no children. He bases his eligibility for nonquota status on his birth in Colombia. Subsequent to the filing of the application it was determined that the alien had married Flor Angela Cortes in accordance with Catholic rites at Bogota, Coloinbia on February 5, 1952. He later married M. Xiomara Cuervo, a permanent resident alien, in Miami, Florida on October 20, 1960. The District Director found that [MASK] was living in a husband and wife relationship with his second spouse</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>On appeal, the DHS contends that the Immigration Judge improperly relied on Matter of Sierra, 26 I&amp;N Dec. 288 (BIA 2014), in support of her determination that [MASK]’s conviction was not a predicate for removal under section 101(a)(43)(G) of the Act. The DHS argues that we limited the applicability of that decision to cases arising in the United States Court of Appeals for the Ninth Circuit. See id. at 290 (“[W]e hold that under the law of the Ninth Circuit, the mental state of ‘reason to believe’ in section 205.273(1) [of the Nevada Revised Statutes] is insufficient for attempted possession of a stolen motor vehicle in violation of [Nevada law] to qualify categorically as an aggravated felony ‘theft offense (including receipt of stolen property).’”). Whether receipt of a stolen motor vehicle under South Dakota law is an aggravated felony is a question of law that we review de novo. 8 C.F.R. § 1003.1(d)(3)(ii) (2017).</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>National Guard’s intelligence unit and could have used this unit to investigate misconduct. Later, as Minister of Defense, [MASK] was the head of the Armed Forces and directed the Security Forces. In this position, he was also a member of President Duarte’s cabinet. Essentially, he was the most powerful person in the military, which was the most powerful force in the country. In fact, [MASK] acknowledged that all members of the Armed Forces were his subordinates and that disobeying any of his orders would result in punishment, including dismissal. He testified that there were no acts of insubordination to any of his orders. According to the record, United States officials considered [MASK] to have command responsibility for his subordinates’ actions. Ambassador White testified that he discussed human rights violations directly with [MASK] in his role as Director of the National Guard. In March 1980, Ambassador White sent a cable to the Secretary of State, in which he related that elements of the Security Forces were conducting a campaign of terror in the countryside and that the command structure of the army and the Security Forces was either tolerating or encouraging the activity. He clarified in his testimony that this “command structure” included [MASK]. An Embassy cable in May 1980 named [MASK] as one of three officials who could “stop the repression if they wished.” On appeal, [MASK] argues that Ambassador White’s testimony supports his assertion that he did not have the ability to stop his subordinates from engaging in torture and extrajudicial killing while he was Director of the National Guard. Ambassador White testified, however, that those in command never claimed that they were unable to control their subordinates. [MASK] also acknowledged that when he was Minister of Defense, he met with Ambassadors Passage and Corr on a weekly or biweekly basis and had additional meetings with other United States officials. He admitted that a perennial subject was the United States officials’ concerns with the democratic process and human rights. Ambassador Passage testified that the United States and President Duarte tasked [MASK] with changing the abusive behavior of the Armed Forces. Ambassador Corr agreed that as Minister of Defense, [MASK] had a duty to investigate allegations of human rights abuses. He also testified that [MASK] had the ability to bring members of the military to justice if they disobeyed a military order, which was demonstrated by [MASK]’s eventual assistance in moving one case to civilian trial. Many times during the war, United States officials identified Salvadoran officials who should be held accountable. During his December 1983 visit, 503</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Decided by Board August 5, 1975 Where [MASK] was admitted to the United States for pernianent residence in possession of a labor certification issued on the basis of representations that he would be employed by the San Francisco Chinese Opera Club as a teacher of musical instruments used in Chinese opera to over 40 pupils at a salary of $650 per month, whereas, in fact; the Club had no job to offer [MASK]; at most, there was an arrangement by which he could give private music lessons to Club members for whatever compensation he might receive; and the purported \"salary\" from the Club was nothing more than a ruse by which he was allowed to submit $650 of his own money, obtained from any source, and receive a check back from the Club \"for the record\", the labor certification was invalid since no certification could have been issued had the correct facts been known. Hence, [MASK]is deportable under section 241(a)(1) of the Immigration and Nationality Act be cause excludable at entry under section 212(a)(14) of the Act for lack of a valid labor certification. CHARGES: Orden Act of 1952—Section 241(a)(1) [8 U.S.C. 1251(a)(1)]—Excludable at time of entry (section 212(a)(14)) [8 U.S.C. 1182(a)(14)I—no labor certification Lodged: Act of 1952—Section 241(a)(1) [8 U.S.C. 1251(a)(1)—Excludable at time of entry (section 212(a)(14)) [8 U.S.C. 1182(a)(14)]—no valid labor certification</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>We also find that [MASK] is not barred from seeking section 212(c) relief by the fact of his conviction for robbery on February 17, 1976. The Service argued at [MASK]'s deportation hearing that this conviction alone ended the period of lawful domicile necessary to a grant of relief under section 212(c) in that it occurred before seven years had elapsed after [MASK]'s entry as a lawful permanent resi6 I. &amp; N. Dec. 392 (BIA 1954; A.G. dent. However, in Matter of 1955), we held that the fact that an alien may have become deportable subsequent to his admission to the United States for lawful permanent residence, without more, was insufficient to preclude him from seeking relief under section 212(c). This position appears to have been followed by the Service in Matter of Mosqueda„ 14 I. &amp; N. Dec. 55 (R.C. 1972). Therefore, we find that the fact of [MASK]'s conviction (providing a basis for deportation) does not in itself terminate [MASK]'s lawful permanent resident status, nor the lawful domicile necessary for a grant of section 212(c) relief We conclude, therefore, that [MASK] is statutorily eligible for such relief. Discretion However, section 212(c) does not provide an indiscriminate waiver for all who demonstrate statutory eligibility for such relief. Instead, the Attorney General or his delegate is required to determine as a matter of discretion whether [MASK] warrants the relief sought. The alien bears the burden of demonstrating that his application merits favorable consideration. Matter of Marin, Interim Decision 2666 (BIA 1978). In Matter of Morin, we established the standards to be applied in considering applications for section 212(c) relief. 4 We observed that confined aliens and those who have recently committed criminal acts will have a more difficult task in showing that discretion should be exercised 4 Contrary to counsel's assertions on appeal, the standards set out in Mann to guide the exercise of discretion in section 212(e) cases are not limited only to those applications involving aliens convicted of drug offenses, but apply equally to an alien such as [MASK], who was convicted of robbery in the third degree.</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Interim Decision #1696 quiry officer for the purpose of affording him an opportunity to reconsider [MASK]s' applications for adjustment of status to that of permanent residents, on the merits. In so doing, we pointed out that the special inquiry officer lacked authority to review [MASK]s' applications for classification as refugees under section 203 (a) (7) of the Immigration and Nationality Act. Despite the foregoing decision of this Board, on remand the special inquiry officer again considered the question of whether [MASK]s were entitled to classification as refugees under section 203(a) (7) of the Immigration and Nationality Act, per langem et latera. On that occasion, however, it was his conclusion that they were entitled to such status. He also found that [MASK]s have established their eligibility for adjustment. of their status to that of permanent</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Interim Decision #2614 mediately.' Whatever the reason for the regulation, it exists, and it was not complied with. The regulation in question is clear. Under S C.F.R. 212.5(b), aliens are entitled to written notice of termination of their parole prior to the institution of exclusion proceedings. Inasmuch as we have found that [MASK]s here were, in fact, paroled into the United States, there is no reason to look behind the clear meaning of the words used hi the regulation. In addition, we note that any parole status is temporary. It makes no difference whether the purpose of the parole ivas to allow [MASK]s to enter while a determination is made on their present applications for asylum, or on a review of a previous grant of asylum, 2 or for some other purpose. The regulations require that before a parole status is revoked, the alien shall receive notice of the Government's intention. We note in passing, that the fact that these aliens resided for long periods in South Vietnam before being evaculated by United States authorities, further complicates the serious legal issues raised if they are not treated as other parolees in like circumstances and parole is sought to be revoked without notice solely because they are not native-</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I. FACTUAL AND PROCEDURAL HISTORY [MASK] is a native and citizen of Peru whose nonimmigrant status was adjusted to that of a lawful permanent resident of the United States on June 4, 2004. On August 5, 2005, she applied for an Illinois driver’s license and signed a voter registration application in which she checked a box indicating that she was a United States citizen. [MASK] filed an application for naturalization dated April 17, 2007, in which she indicated that she had registered to vote and had voted in an election in the United States. During an interview regarding her application, she disclosed that she had voted in 2006. On January 11, 2008, the Department of Homeland Security (“DHS”) issued a notice to appear, alleging that [MASK] voted in the general election in Illinois on November 7, 2006, in violation of 18 U.S.C. § 611 (2006), and charging her with removability under section 237(a)(6) of the Immigration and Nationality Act, 8 U.S.C. § 1227(a)(6) (2006), as an alien 1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_random_elements(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnjDIuQ3IrI-"
      },
      "source": [
        "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o4rUteaIrI_",
        "outputId": "53017139-ac23-4269-d1d5-296b50c943c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
              "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
              "Args:\n",
              "    predictions: list of predictions to score.\n",
              "        Each translation should be tokenized into a list of tokens.\n",
              "    references: list of lists of references for each translation.\n",
              "        Each reference should be tokenized into a list of tokens.\n",
              "Returns: depending on the GLUE subset, one or several of:\n",
              "    \"accuracy\": Accuracy\n",
              "    \"f1\": F1 score\n",
              "    \"pearson\": Pearson Correlation\n",
              "    \"spearmanr\": Spearman Correlation\n",
              "    \"matthews_correlation\": Matthew Correlation\n",
              "Examples:\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'accuracy': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'accuracy': 1.0, 'f1': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
              "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
              "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
              "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
              "\n",
              "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'matthews_correlation': 1.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "You can call its `compute` method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XN1Rq0aIrJC",
        "outputId": "719d18bd-124d-4560-997b-84e155879693"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.546875, 'f1': 0.5797101449275361}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "fake_preds = np.random.randint(0, 2, size=(64,))\n",
        "fake_labels = np.random.randint(0, 2, size=(64,))\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOCrQwPoIrJG"
      },
      "source": [
        "Note that `load_metric` has loaded the proper metric associated to your task, which is:\n",
        "\n",
        "- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
        "- for MNLI (matched or mismatched): Accuracy\n",
        "- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for QNLI: Accuracy\n",
        "- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for RTE: Accuracy\n",
        "- for SST-2: Accuracy\n",
        "- for STS-B: [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Spearman's_Rank_Correlation_Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
        "- for WNLI: Accuracy\n",
        "\n",
        "so the metric object only computes the one(s) needed for your task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the 🤗 Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5hBlsrHIrJL",
        "outputId": "fe69a5f8-0152-485e-9024-5644bf2ed5b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [0, 31414, 6, 42, 65, 3645, 328, 2, 2, 2409, 42, 3645, 1411, 19, 24, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_0B1M2IrJM"
      },
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyGdtK9oIrJM"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "    \"eoir_privacy\" : (\"text\", None)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbqtC4MrIrJO"
      },
      "source": [
        "We can double check it does work on our current dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19GG646uIrJO",
        "outputId": "d30b1682-e4e1-45a4-f2bb-5b5b7265c59e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: \fheld that a prosecution for violation of a city ordinance, while in the form of a criminal prosecution, is, in fact, a civil proceeding to recover a penalty and a preponderance of the evidence is all that is required to sustain a conviction.' He concludes that the criminal . section 241(a) (4) is not sustained as a matter charge laid unticr of law. We do not concur. Here we are concerned with interpreting a Federal statute in which Congress has expressed its disapproval of the type of behavior for which [MASK] was convicted under a city ordinance in the State of Nebraska. The fact that the misconduct is considered a civil proceeding by the courts of that state does not control when interpreting the immigration laws. Au act of Congress is not circumscribed by restrictive holdings of state courts defining the jurisdictional and procedural limits of inferior courts of criminal jurisdiction.\n"
          ]
        }
      ],
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "if sentence2_key is None:\n",
        "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
        "else:\n",
        "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
        "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can them write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    if sentence2_key is None:\n",
        "        return tokenizer(examples[sentence1_key], truncation=True)\n",
        "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b70jh26IrJS",
        "outputId": "b95e3ca7-9523-487f-ec45-935941f58b17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 50120, 11706, 14, 10, 6914, 13, 4565, 9, 10, 343, 11020, 6, 150, 11, 5, 1026, 9, 10, 1837, 6914, 6, 16, 6, 11, 754, 6, 10, 2366, 19635, 7, 5312, 10, 2861, 8, 10, 12760, 17037, 2389, 9, 5, 1283, 16, 70, 14, 16, 1552, 7, 9844, 10, 6866, 955, 91, 14372, 14, 5, 1837, 479, 2810, 35752, 1640, 102, 43, 36, 306, 43, 16, 45, 5232, 25, 10, 948, 1427, 4976, 7587, 636, 338, 9, 488, 4, 166, 109, 45, 10146, 710, 4, 1398, 52, 32, 2273, 19, 37212, 10, 1853, 16840, 11, 61, 1148, 34, 2327, 63, 32129, 9, 5, 1907, 9, 3650, 13, 61, 646, 32804, 530, 742, 21, 3828, 223, 10, 343, 11020, 11, 5, 331, 9, 8688, 4, 20, 754, 14, 5, 6046, 16, 1687, 10, 2366, 19635, 30, 5, 4354, 9, 14, 194, 473, 45, 797, 77, 37212, 5, 2447, 2074, 4, 14870, 1760, 9, 1148, 16, 45, 38529, 31623, 30, 23680, 4582, 9, 194, 4354, 17032, 5, 44697, 41247, 8, 24126, 4971, 9, 28510, 4354, 9, 1837, 10542, 4, 2], [0, 27387, 7, 646, 32804, 530, 46117, 29, 6866, 187, 5, 2970, 9, 22, 13713, 2553, 784, 9636, 18199, 113, 25, 6533, 30, 5, 11020, 259, 223, 1701, 6083, 1499, 16, 67, 156, 1837, 30, 8749, 971, 6, 2810, 29600, 6, 41114, 15420, 11282, 9, 8688, 6, 30190, 4, 85, 16, 5, 737, 9, 5, 10294, 1841, 6, 11, 97, 1617, 6, 14, 1712, 5, 9082, 34, 42304, 3446, 7, 5, 3647, 9, 10, 78, 12, 4684, 343, 7, 19074, 12618, 4756, 223, 343, 31840, 6, 21507, 5, 1785, 9, 221, 1120, 1790, 34, 1029, 16435, 17355, 10542, 147, 5, 11020, 67, 28978, 10, 4565, 9, 10, 194, 1837, 16840, 4, 166, 109, 45, 2854, 19, 42, 13794, 3486, 109, 52, 465, 14, 5, 3446, 579, 4418, 30, 5, 14951, 1036, 4548, 39, 737, 4, 85, 16, 2966, 14, 5, 4418, 2390, 2559, 343, 31840, 88, 80, 6363, 6, 13953, 6, 36, 134, 43, 13888, 156, 1837, 30, 194, 488, 6, 8, 36, 176, 43, 2366, 7069, 7, 5312, 10, 2861, 4, 20, 4418, 1200, 2212, 12618, 13, 6165, 9, 343, 31840, 2273, 19, 9473, 876, 2533, 2883, 6, 6780, 10, 385, 636, 3109, 8344, 352, 790, 8, 19289, 37187, 7, 29196, 15, 5, 11457, 4, 20, 2124, 837, 9, 8688, 11, 349, 4327, 547, 14, 5, 6914, 21, 10, 2366, 8, 45, 10, 1837, 19635, 8, 6, 3891, 6, 5, 6914, 56, 7, 3364, 63, 403, 30, 129, 10, 12760, 17037, 2389, 9, 5, 1283, 4, 20, 5135, 11, 5, 5086, 7, 5, 754, 14, 5, 6165, 58, 45, 156, 41, 2970, 30, 194, 488, 32, 28700, 102, 4, 166, 1591, 14, 11, 5, 3864, 15677, 403, 36, 35901, 763, 6, 41887, 231, 43, 5, 2970, 9, 6780, 10, 23547, 790, 16, 41, 2970, 156, 1837, 30, 194, 488, 36, 22580, 971, 12, 36991, 6, 13456, 9, 10, 23547, 790, 2181, 9, 10, 26575, 131, 2810, 1132, 5579, 466, 1360, 6, 2970, 156, 1837, 322, 166, 67, 1591, 14, 11, 5, 4936, 12105, 2], [0, 50120, 27216, 196, 11, 5, 638, 4, 166, 1591, 14, 646, 32804, 530, 742, 34, 41, 4935, 2237, 638, 13, 5, 675, 34094, 149, 24712, 4, 166, 67, 1591, 14, 5, 638, 473, 45, 5585, 10, 595, 266, 9, 10, 2048, 803, 4, 2096, 5, 4215, 52, 40, 22992, 507, 568, 11, 5, 403, 8, 40, 6398, 463, 13, 10, 14015, 1576, 15, 5, 696, 9, 25159, 3500, 6, 5, 638, 7, 28, 1950, 2226, 19, 6203, 7, 1857, 14, 646, 32804, 530, 742, 16, 23311, 7, 10079, 4835, 8, 34, 56, 4158, 19, 5151, 97, 87, 39, 1141, 4, 83, 266, 9, 10, 595, 2048, 803, 16, 7, 28, 156, 10, 233, 9, 5, 638, 8, 5, 403, 1835, 7, 5, 1785, 9, 10294, 13248, 13, 507, 568, 4, 660, 3901, 645, 40, 28, 2867, 4, 9729, 35, 85, 16, 3660, 14, 5, 403, 28, 6398, 13833, 7, 5, 780, 6422, 1036, 13, 10, 14015, 1576, 11, 10753, 19, 5, 37902, 2979, 4, 2], [0, 133, 2194, 9, 41, 70, 225, 54, 21, 39896, 2641, 7, 5, 315, 532, 25, 10, 33954, 35832, 786, 28089, 8, 54, 16, 3348, 7, 3014, 14, 2194, 189, 28, 5493, 30, 5, 2745, 1292, 11, 39, 14145, 36, 5087, 215, 3478, 25, 37, 189, 30871, 7, 36702, 5, 2502, 9, 42, 17818, 9382, 7, 5, 1200, 9, 20739, 54, 2867, 5, 315, 532, 11, 205, 3975, 25, 786, 13255, 24280, 43, 7, 14, 9, 41, 13058, 39896, 2641, 13, 4398, 5238, 5951, 10, 20248, 15192, 118, 12, 4470, 50, 25, 10, 786, 2253, 6804, 8910, 223, 2810, 6560, 1640, 102, 21704, 2518, 21704, 250, 238, 114, 36, 134, 43, 5, 13058, 817, 2502, 13, 13380, 6, 36, 176, 43, 5, 13058, 16, 2329, 36507, 7, 5, 315, 532, 13, 4398, 5238, 223, 42, 1783, 6, 36, 246, 43, 10, 20248, 50, 786, 2253, 6804, 8910, 8915, 21, 1320, 577, 7, 123, 23, 5, 86, 9, 39, 2502, 13, 13380, 6, 36, 306, 43, 10, 20248, 50, 786, 2253, 6804, 8910, 8915, 16, 1320, 22345, 12, 441, 7, 123, 23, 5, 86, 39, 2502, 16, 2033, 6, 8, 36, 245, 43, 114, 4564, 10, 786, 2253, 6804, 2194, 223, 2810, 6560, 1640, 102, 43, 36, 2518, 43, 36, 250, 43, 37, 34, 57, 11, 5, 315, 532, 13, 23, 513, 65, 76, 2052, 7, 6819, 14, 2194, 4, 83, 20248, 8910, 8915, 5658, 28, 1687, 1320, 577, 13, 5, 6216, 9, 42, 45845, 129, 114, 5, 4745, 9, 5, 20248, 7, 61, 5, 13058, 16, 1427, 868, 1534, 18983, 1792, 12, 6821, 1452, 196, 30, 646, 32804, 530, 742, 29, 3382, 15, 10, 7407, 8244, 2445, 889, 4, 5053, 13058, 54, 5658, 2870, 41, 2502, 13, 13380, 9, 39, 2194, 223, 42, 2810, 5658, 12679, 22335, 39, 786, 28089, 2194, 4, 2], [0, 1121, 1285, 7, 5, 3280, 3471, 9, 42, 6397, 9, 488, 8, 5, 9704, 3478, 6, 52, 33, 1552, 14, 646, 32804, 530, 742, 5242, 205, 7654, 2048, 4, 1990, 5, 195, 107, 2052, 7, 5, 3386, 9, 39, 2502, 25, 10, 5701, 8, 4692, 1453, 6, 566, 643, 6, 13, 13684, 549, 5, 2745, 1292, 17, 27, 29, 14145, 5658, 28, 1931, 12, 3335, 48843, 11, 10, 1989, 403, 36, 448, 7933, 9, 289, 18242, 500, 18242, 6, 83, 12, 698, 3761, 3170, 2881, 6, 262, 38, 4, 359, 234, 4, 1502, 4, 231, 4708, 322, 616, 2810, 6560, 1640, 506, 43, 9, 5, 1760, 17410, 579, 621, 31, 145, 11394, 25, 10, 621, 9, 205, 7654, 2048, 114, 6, 148, 5, 675, 13, 61, 205, 7654, 2048, 16, 1552, 7, 28, 2885, 6, 37, 34, 2021, 42136, 6, 41, 646, 32804, 530, 742, 13, 9160, 12, 12906, 9, 2194, 223, 2810, 28328, 16, 45, 1198, 26441, 30, 14, 2810, 31, 10584, 205, 7654, 2048, 25, 5063, 5, 488, 3486, 5, 9704, 3478, 680, 215, 10, 7404, 36, 448, 7933, 9, 234, 18242, 6, 83, 12, 1898, 17655, 5339, 6, 262, 38, 4, 359, 234, 4, 1502, 4, 42100, 322, 635, 6, 6077, 197, 28, 576, 7, 5, 2820, 278, 7264, 11, 2810, 6560, 1640, 506, 43, 11, 10922, 12, 4481, 549, 646, 32804, 530, 742, 16, 11314, 9, 5, 3500, 5372, 30, 123, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess_function(dataset['train'][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d177a619eca743de902a1df9ec6f07c9",
            "752972d4d334442da43bddaf3b30dcee",
            "e51590d7574b438d80904e6c36661b2f",
            "dbd2a309ee6940ff9bb9bec47c42cc06",
            "08b2481d95ac4141a6f2ab406ea2e072",
            "3377fc74b77b487cac102b3f1f386ed1",
            "84d366d59f3f4f40b604ab70675752c9",
            "2bc573fb42724d7c969d3c7cff6f8858",
            "cfae6bd55bf74126a0df0ec5f499060f",
            "e6201e5aee0c4414bad27ea32ae39a92",
            "1b4e320834574e8384ddbb7a4a65acdb",
            "d7917287dabe4a90bf28a9ea699f2ff0",
            "f9b1270cf7cf4ce0878261f3f1e289c0",
            "b146eb35f6f341c6a020fa951b4a5172",
            "af8407a8a80e40359c3d51d0d283c628",
            "f8bd2e7d752f47c483ebc2470cb6b9c1",
            "fd1c31f8d3b6463fb75f25ecf7879858",
            "fc5528ed77f144b1bc69e2fbc608922b",
            "e3c01649f0744b2a8b3b766edb911c26",
            "34f81b862a65455388b4fbfcdac841f5",
            "f2d8b839e1cb4b799b03d46277c7b446",
            "073920e2296d484696b7eeb755d989b1"
          ]
        },
        "id": "DDtsaJeVIrJT",
        "outputId": "975c1974-8876-4055-a0f1-7a9c7f27f8de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d177a619eca743de902a1df9ec6f07c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7917287dabe4a90bf28a9ea699f2ff0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "encoded_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "Even better, the results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. 🤗 Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `AutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlqNaB8jIrJW",
        "outputId": "53f553d3-b709-4e59-982a-840b3660b0ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels, gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define two more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    gradient_accumulation_steps =16,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oGyxouK7_1jq",
        "outputId": "7d7bda09-94be-421e-b4b4-fec0a940f038"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'longformer-base-4096-finetuned-eoir_privacy'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f\"{model_name}-finetuned-{task}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the `Trainer` to load the best model it saved (according to `metric_name`) at the end of training.\n",
        "\n",
        "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/bert-finetuned-mrpc\"` or `\"huggingface/bert-finetuned-mrpc\"`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "The last thing to define for our `Trainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    if task != \"stsb\":\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "    else:\n",
        "        predictions = predictions[:, 0]\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga_oxA-y3_BR",
        "outputId": "fcc2b90e-0607-451d-f34d-a196668e19e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train']['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[validation_key],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibWGmvxbIrJg"
      },
      "source": [
        "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it once last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a pad method that will do all of this right for us, and the `Trainer` will use it. You can customize this part by defining and passing your own `data_collator` which will receive the samples like the dictionaries seen above and will need to return a dictionary of tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uNx5pyRlIrJh",
        "outputId": "75b0dbda-2176-4e14-c95b-aa242c20b21f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 6309\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 147\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='109' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [109/147 1:57:53 < 41:52, 0.02 it/s, Epoch 2.20/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.461579</td>\n",
              "      <td>0.788660</td>\n",
              "      <td>0.338710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.353126</td>\n",
              "      <td>0.849227</td>\n",
              "      <td>0.623794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 891 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 661 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 864 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 758 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 774 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 765 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 816 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 850 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 890 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 778 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 750 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 553 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 854 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 805 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 689 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 855 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 887 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 743 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 770 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 804 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 661 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 851 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 832 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 743 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 788 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 1113 to 1536 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 497 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 828 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 778 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 743 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 796 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 707 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 802 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 855 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 840 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 736 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1552\n",
            "  Batch size = 8\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 736 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 839 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 819 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 820 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 819 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 813 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 821 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 972 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 712 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Saving model checkpoint to longformer-base-4096-finetuned-eoir_privacy/checkpoint-49\n",
            "Configuration saved in longformer-base-4096-finetuned-eoir_privacy/checkpoint-49/config.json\n",
            "Model weights saved in longformer-base-4096-finetuned-eoir_privacy/checkpoint-49/pytorch_model.bin\n",
            "tokenizer config file saved in longformer-base-4096-finetuned-eoir_privacy/checkpoint-49/tokenizer_config.json\n",
            "Special tokens file saved in longformer-base-4096-finetuned-eoir_privacy/checkpoint-49/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 743 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 850 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 707 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 804 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 486 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 489 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 864 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 816 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 890 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 689 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 778 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 832 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 1113 to 1536 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 891 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 303 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 778 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 840 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 805 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 807 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 736 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 855 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 788 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 488 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 475 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 851 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 855 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 524 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 828 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 758 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 854 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 887 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 581 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 661 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 802 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 661 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 770 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 579 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 743 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 774 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 750 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 557 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1552\n",
            "  Batch size = 8\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 744 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 520 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 615 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 293 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 567 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 543 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 621 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 308 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 571 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 519 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 736 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 628 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 839 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 819 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 577 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 820 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 819 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 813 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 821 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 972 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 691 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 712 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Saving model checkpoint to longformer-base-4096-finetuned-eoir_privacy/checkpoint-98\n",
            "Configuration saved in longformer-base-4096-finetuned-eoir_privacy/checkpoint-98/config.json\n",
            "Model weights saved in longformer-base-4096-finetuned-eoir_privacy/checkpoint-98/pytorch_model.bin\n",
            "tokenizer config file saved in longformer-base-4096-finetuned-eoir_privacy/checkpoint-98/tokenizer_config.json\n",
            "Special tokens file saved in longformer-base-4096-finetuned-eoir_privacy/checkpoint-98/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 617 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 636 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 435 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 1113 to 1536 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 774 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 521 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 816 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 743 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 450 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 802 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512\n",
            "Initializing global attention on CLS token...\n",
            "Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKASz-2vIrJi"
      },
      "source": [
        "We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOUcBkX8IrJi"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RwlZIa0B0VQ"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iBR_-5_BpuW"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline# load from previously saved model\n",
        "pipe = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-eoir_privacy\", tokenizer=\"distilbert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjofv9jwB50z"
      },
      "outputs": [],
      "source": [
        "pipe(\"[MASK] asylum immigration judge minor violence against women act\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP-VQOyIrJk"
      },
      "source": [
        "To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC-3RoBl1fKz"
      },
      "source": [
        "You can now upload the result of the training to the Hub, just execute this instruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnMjY3as1fK0"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpLbtqzI1fK0"
      },
      "source": [
        "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"sgugger/my-awesome-model\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k8ge1L1IrJk"
      },
      "source": [
        "## Hyperparameter search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNfajuw_IrJl"
      },
      "source": [
        "The `Trainer` supports hyperparameter search using [optuna](https://optuna.org/) or [Ray Tune](https://docs.ray.io/en/latest/tune/). For this last section you will need either of those libraries installed, just uncomment the line you want on the next cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUdakNBhIrJl"
      },
      "outputs": [],
      "source": [
        "# ! pip install optuna\n",
        "# ! pip install ray[tune]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttfT0CqaIrJm"
      },
      "source": [
        "During hyperparameter search, the `Trainer` will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. We jsut use the same function as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sgjdLKcIrJm"
      },
      "outputs": [],
      "source": [
        "# def model_init():\n",
        "#     return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMXfVJO4IrJo"
      },
      "source": [
        "And we can instantiate our `Trainer` like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71pt6N0eIrJo"
      },
      "outputs": [],
      "source": [
        "# trainer = Trainer(\n",
        "#     model_init=model_init,\n",
        "#     args=args,\n",
        "#     train_dataset=encoded_dataset[\"train\"],\n",
        "#     eval_dataset=encoded_dataset[validation_key],\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQxrzFP4IrJq"
      },
      "source": [
        "The method we call this time is `hyperparameter_search`. Note that it can take a long time to run on the full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the training dataset by replacing the `train_dataset` line above by:\n",
        "```python\n",
        "train_dataset = encoded_dataset[\"train\"].shard(index=1, num_shards=10) \n",
        "```\n",
        "for 1/10th of the dataset. Then you can run a full training on the best hyperparameters picked by the search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NboJ7kDOIrJq"
      },
      "outputs": [],
      "source": [
        "# best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUTD72qCIrJs"
      },
      "source": [
        "The `hyperparameter_search` method returns a `BestRun` objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psi4JymeIrJs"
      },
      "outputs": [],
      "source": [
        "# best_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdjWbRIIrJu"
      },
      "source": [
        "You can customize the objective to maximize by passing along a `compute_objective` function to the `hyperparameter_search` method, and you can customize the search space by passing a `hp_space` argument to `hyperparameter_search`. See this [forum post](https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10) for some examples.\n",
        "\n",
        "To reproduce the best training, just set the hyperparameters in your `TrainingArgument` before creating a `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsJ6sqdGIrJu"
      },
      "outputs": [],
      "source": [
        "# for n, v in best_run.hyperparameters.items():\n",
        "#     setattr(trainer.args, n, v)\n",
        "\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOeNRH9d1fK5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7k8ge1L1IrJk"
      ],
      "name": "EOIR",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00858ad831ad45c9b810b95c22a0b3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_362390504df94a7ab41aa1093b65a482",
            "placeholder": "​",
            "style": "IPY_MODEL_e1f79ff76dcc4bcfa92b2347b1df0af7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "073920e2296d484696b7eeb755d989b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08b2481d95ac4141a6f2ab406ea2e072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08cc930333754a7d9a2d2d8bf301b8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_417d3e7c4d974a31b33a4a74ab2532e9",
            "placeholder": "​",
            "style": "IPY_MODEL_f702d59ce3af429fbb464b9d768a1d14",
            "value": "Downloading data files: 100%"
          }
        },
        "113ce950efcd473e869797346ab74b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cf7cb7c544a459797406d42cc219cec",
              "IPY_MODEL_c4651d73a7f54ab284c6392744eded7f",
              "IPY_MODEL_5e22620dcdef4e62b71f08e711eb78db"
            ],
            "layout": "IPY_MODEL_d505bd73227c49668c298a2d2bcbc6c0"
          }
        },
        "14dd88362bf84377938b5d3b47a48f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186aa4cf86a34f4b9ba1e829778ee58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b4e320834574e8384ddbb7a4a65acdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c4768f80a594073a0c5db872c1a17f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a01301bfa834fe9a3ed0e970b135dec",
            "placeholder": "​",
            "style": "IPY_MODEL_c825a55402574fada73d3c870ea6536f",
            "value": " 6181/6309 [00:01&lt;00:00, 4848.41 examples/s]"
          }
        },
        "211e7a4f3a554d8495bc064d9d708a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "23cac59fa57940aa8f63ab564136f7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6943ed1569b46f498af50c7cf195f44",
              "IPY_MODEL_658cff5ea4eb42078963d2a227a4e647",
              "IPY_MODEL_a18390342e47484c946fabf46e716515"
            ],
            "layout": "IPY_MODEL_c975b61136c94b67a0e857e80f056d7b"
          }
        },
        "282ec4abcc39445ba42e4bbc58cff5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b56e454602941dda8fc6622eac051b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d3d1ee4d20407fbd6c29c7bc1b4bb3",
            "placeholder": "​",
            "style": "IPY_MODEL_31222f685f844ed488e40b24b071e9fe",
            "value": "Generating validation split: 100%"
          }
        },
        "2bc573fb42724d7c969d3c7cff6f8858": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c387ea80fe643d48c4dfdfd04eb160e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b56bb14b8d741a9ace5579f71408b82",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6be8ecc7b024b97937ed691b2235dc8",
            "value": 1
          }
        },
        "2c6f1984fc594dbfb2f9dd6a7642b13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cf7cb7c544a459797406d42cc219cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14dd88362bf84377938b5d3b47a48f9c",
            "placeholder": "​",
            "style": "IPY_MODEL_f61338d982704978bea80ff3fe2aadd7",
            "value": "100%"
          }
        },
        "2ff528c8f7e54e8298ed407defdcff3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31222f685f844ed488e40b24b071e9fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "324cfb7794ec4e42902a32470ffe456a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3377fc74b77b487cac102b3f1f386ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34d6c317a04d4139a3dcae157dae828f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34f81b862a65455388b4fbfcdac841f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "362390504df94a7ab41aa1093b65a482": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372b611c73a44c36b64df1922b108c02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a01301bfa834fe9a3ed0e970b135dec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da93abdc68c42e0accdb025f774aa1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bdb57458ae346399b52a5d71aea0df6",
            "placeholder": "​",
            "style": "IPY_MODEL_668c82df4a0e4d5ab275942e9dc033f2",
            "value": "Downloading builder script: 100%"
          }
        },
        "403c4b7a80c5451597814592797fa7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4140bd879ef64c378e723f515e1dcc7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "417d3e7c4d974a31b33a4a74ab2532e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4340593483c14282841abd25fee8f86a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "476e4f3647a045c5a014d345ce91865b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48180c0d6f8645b087ca99f1583e4e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2528fea8054ddc88c13f0db936dc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b313a595e5545f092ff5b8634166e14",
            "placeholder": "​",
            "style": "IPY_MODEL_2c6f1984fc594dbfb2f9dd6a7642b13a",
            "value": " 2.76k/2.76k [00:00&lt;00:00, 76.9kB/s]"
          }
        },
        "4b313a595e5545f092ff5b8634166e14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bdb57458ae346399b52a5d71aea0df6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "531d9e4d265841839336711a5c4a1eeb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5360d869ee2342cbaad6d4df3f7dc576": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578e7e15451f41969e0f30a822c1ae96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372b611c73a44c36b64df1922b108c02",
            "max": 1892108,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b77ad20018524ec69241a6100cde18ef",
            "value": 1892108
          }
        },
        "58b2e4bea7a64c3bbde79a2d5d43f7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a7785a920f14b2a84f3e8e917f02c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e22620dcdef4e62b71f08e711eb78db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb83a716f28a469b89cb85011327ef9d",
            "placeholder": "​",
            "style": "IPY_MODEL_f22cac22604f441ca818c3c3aa42aa42",
            "value": " 2/2 [00:00&lt;00:00,  9.41it/s]"
          }
        },
        "6042203aa65f4456a45732ea7f737d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64e2250b867944bd87e82bc9b7311ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3da93abdc68c42e0accdb025f774aa1a",
              "IPY_MODEL_cf300dc673c744d9957a00c7167aafee",
              "IPY_MODEL_4b2528fea8054ddc88c13f0db936dc7b"
            ],
            "layout": "IPY_MODEL_282ec4abcc39445ba42e4bbc58cff5c3"
          }
        },
        "6508db387816439cb41ddc704b879249": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "658cff5ea4eb42078963d2a227a4e647": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_772fb7fac1744e65b4ec4e6a3f86e043",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_186aa4cf86a34f4b9ba1e829778ee58e",
            "value": 1
          }
        },
        "664d7c2c34c344c4813f9a3a7e9526f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00858ad831ad45c9b810b95c22a0b3b8",
              "IPY_MODEL_e156e839e650459ca209b985b1a01914",
              "IPY_MODEL_cd1e3f1581e8431a9710cf90d8b24388",
              "IPY_MODEL_72af77165851496fb0db8d2676974d59",
              "IPY_MODEL_7fd3275f9caa40a8a860a0763c73aa64"
            ],
            "layout": "IPY_MODEL_6f2b6d1924164d6d8c4a4ecb5f959057"
          }
        },
        "66724752df9944ae8d04377e162e9a88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "668c82df4a0e4d5ab275942e9dc033f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dac1446a8c04e3c9393cefc98e32b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f2b6d1924164d6d8c4a4ecb5f959057": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "72af77165851496fb0db8d2676974d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d92c2d7c10b24fcd95e1cec9ebcea0e5",
            "placeholder": "​",
            "style": "IPY_MODEL_403c4b7a80c5451597814592797fa7b7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. <br> <i>Logging in with your username and password is deprecated and\nwon't be possible anymore in the near future. You can still use them for now by\nclicking below.</i> </center>"
          }
        },
        "752972d4d334442da43bddaf3b30dcee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3377fc74b77b487cac102b3f1f386ed1",
            "placeholder": "​",
            "style": "IPY_MODEL_84d366d59f3f4f40b604ab70675752c9",
            "value": "100%"
          }
        },
        "75556157e2074b9aa90606ea9e9da618": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c48177d5b3d04817ba71fc3f12450616",
            "max": 6309,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4140bd879ef64c378e723f515e1dcc7e",
            "value": 6309
          }
        },
        "75b9061d9ef24b56915b44230eea44c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76a025fdfd5e44ec8c73ca19f1d2f47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbcd6f92c2884b4284177b3051e6280e",
            "max": 1552,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b96a1e56b1954b96a52baa5d91b801f1",
            "value": 1552
          }
        },
        "772fb7fac1744e65b4ec4e6a3f86e043": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ab855ab579d42d285c2cfa58796ba21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ddb172f0e954414a317a16a1d6fe31c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f7a6a4d75df4d08af50bac590c84a59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd3275f9caa40a8a860a0763c73aa64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Use password",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_dc2d92ec876e45bfaac882031e580161",
            "style": "IPY_MODEL_324cfb7794ec4e42902a32470ffe456a",
            "tooltip": ""
          }
        },
        "84d366d59f3f4f40b604ab70675752c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85dbdb4fb3cc48659c37976db5b6ac78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66724752df9944ae8d04377e162e9a88",
            "placeholder": "​",
            "style": "IPY_MODEL_7ab855ab579d42d285c2cfa58796ba21",
            "value": "Generating train split:  98%"
          }
        },
        "85e0a2a479bf4b5f9a6cfa99685bb7ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87677cac852240a8ae27b4171c9301a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3aa20e336fb4a5290dc6e424c1fdbb4",
              "IPY_MODEL_578e7e15451f41969e0f30a822c1ae96",
              "IPY_MODEL_a4a07ee286414788a77beb3ee3fbb2cc"
            ],
            "layout": "IPY_MODEL_476e4f3647a045c5a014d345ce91865b"
          }
        },
        "88b011ba7165406d8c00a971011b739c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85e0a2a479bf4b5f9a6cfa99685bb7ab",
            "max": 528016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dac1446a8c04e3c9393cefc98e32b00",
            "value": 528016
          }
        },
        "8d352e05994f408abbfb6bc12afd3764": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c65d50e153a541ac974ff2c2445ac78c",
            "placeholder": "​",
            "style": "IPY_MODEL_f1aff7ea074948a1a5256c33bfb632ff",
            "value": "Downloading data: 100%"
          }
        },
        "930769b32169425bbb26f61f17440d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d352e05994f408abbfb6bc12afd3764",
              "IPY_MODEL_88b011ba7165406d8c00a971011b739c",
              "IPY_MODEL_c5f078557e214d599c5c74c9694957b3"
            ],
            "layout": "IPY_MODEL_c69ae8ba2be14644ba97e519100bb372"
          }
        },
        "98a00fe3243a4a5ab572634ddec68a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_531d9e4d265841839336711a5c4a1eeb",
            "placeholder": "​",
            "style": "IPY_MODEL_6508db387816439cb41ddc704b879249",
            "value": " 1551/1552 [00:00&lt;00:00, 3693.62 examples/s]"
          }
        },
        "9b56bb14b8d741a9ace5579f71408b82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9f23b42a6548758b81147ac31bf5a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f490d02ca274614b5cf3a2ce670a1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ff528c8f7e54e8298ed407defdcff3b",
            "placeholder": "​",
            "style": "IPY_MODEL_9d9f23b42a6548758b81147ac31bf5a2",
            "value": " 1/1 [00:01&lt;00:00,  1.09s/it]"
          }
        },
        "a18390342e47484c946fabf46e716515": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ddb172f0e954414a317a16a1d6fe31c",
            "placeholder": "​",
            "style": "IPY_MODEL_58b2e4bea7a64c3bbde79a2d5d43f7d0",
            "value": " 1/1 [00:00&lt;00:00,  1.05it/s]"
          }
        },
        "a4a07ee286414788a77beb3ee3fbb2cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48180c0d6f8645b087ca99f1583e4e2f",
            "placeholder": "​",
            "style": "IPY_MODEL_a9506157b54b4114a5b622889b4ffadc",
            "value": " 1.89M/1.89M [00:00&lt;00:00, 5.00MB/s]"
          }
        },
        "a9506157b54b4114a5b622889b4ffadc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa1f9f2ea87b4ad5a3592f18b62da0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae43fdaf988e4f1ba7a72acbfd198de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af8407a8a80e40359c3d51d0d283c628": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2d8b839e1cb4b799b03d46277c7b446",
            "placeholder": "​",
            "style": "IPY_MODEL_073920e2296d484696b7eeb755d989b1",
            "value": " 2/2 [00:01&lt;00:00,  1.87ba/s]"
          }
        },
        "b146eb35f6f341c6a020fa951b4a5172": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3c01649f0744b2a8b3b766edb911c26",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34f81b862a65455388b4fbfcdac841f5",
            "value": 2
          }
        },
        "b731387ad93c499691167c4b77ae2811": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b77ad20018524ec69241a6100cde18ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7f6c5d7bc7f4df8a40ce983dd7c65e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b96a1e56b1954b96a52baa5d91b801f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbcd6f92c2884b4284177b3051e6280e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4651d73a7f54ab284c6392744eded7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9746194f25428d92ccfefaf972b01a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3024bfb689240828653eba11bbb70d3",
            "value": 2
          }
        },
        "c48177d5b3d04817ba71fc3f12450616": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f078557e214d599c5c74c9694957b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd9cbf26f7bf484795f8193a62098c69",
            "placeholder": "​",
            "style": "IPY_MODEL_ae43fdaf988e4f1ba7a72acbfd198de3",
            "value": " 528k/528k [00:00&lt;00:00, 2.42MB/s]"
          }
        },
        "c65d50e153a541ac974ff2c2445ac78c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c69ae8ba2be14644ba97e519100bb372": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6be8ecc7b024b97937ed691b2235dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c825a55402574fada73d3c870ea6536f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c975b61136c94b67a0e857e80f056d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd1e3f1581e8431a9710cf90d8b24388": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6042203aa65f4456a45732ea7f737d7c",
            "style": "IPY_MODEL_211e7a4f3a554d8495bc064d9d708a07",
            "tooltip": ""
          }
        },
        "ce27eb1a6a074360adeeaa42c0351c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85dbdb4fb3cc48659c37976db5b6ac78",
              "IPY_MODEL_75556157e2074b9aa90606ea9e9da618",
              "IPY_MODEL_1c4768f80a594073a0c5db872c1a17f3"
            ],
            "layout": "IPY_MODEL_cf555a4443d34486abae8560abd9f823"
          }
        },
        "ceb6b3e11ea8473792231297e924c16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08cc930333754a7d9a2d2d8bf301b8cf",
              "IPY_MODEL_2c387ea80fe643d48c4dfdfd04eb160e",
              "IPY_MODEL_9f490d02ca274614b5cf3a2ce670a1db"
            ],
            "layout": "IPY_MODEL_dbd93cf11f4643c89ca14ace4bf93498"
          }
        },
        "cf300dc673c744d9957a00c7167aafee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75b9061d9ef24b56915b44230eea44c0",
            "max": 2756,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa1f9f2ea87b4ad5a3592f18b62da0d2",
            "value": 2756
          }
        },
        "cf555a4443d34486abae8560abd9f823": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfae6bd55bf74126a0df0ec5f499060f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d177a619eca743de902a1df9ec6f07c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_752972d4d334442da43bddaf3b30dcee",
              "IPY_MODEL_e51590d7574b438d80904e6c36661b2f",
              "IPY_MODEL_dbd2a309ee6940ff9bb9bec47c42cc06"
            ],
            "layout": "IPY_MODEL_08b2481d95ac4141a6f2ab406ea2e072"
          }
        },
        "d1d3d1ee4d20407fbd6c29c7bc1b4bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d505bd73227c49668c298a2d2bcbc6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7917287dabe4a90bf28a9ea699f2ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9b1270cf7cf4ce0878261f3f1e289c0",
              "IPY_MODEL_b146eb35f6f341c6a020fa951b4a5172",
              "IPY_MODEL_af8407a8a80e40359c3d51d0d283c628"
            ],
            "layout": "IPY_MODEL_f8bd2e7d752f47c483ebc2470cb6b9c1"
          }
        },
        "d92c2d7c10b24fcd95e1cec9ebcea0e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9746194f25428d92ccfefaf972b01a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd2a309ee6940ff9bb9bec47c42cc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6201e5aee0c4414bad27ea32ae39a92",
            "placeholder": "​",
            "style": "IPY_MODEL_1b4e320834574e8384ddbb7a4a65acdb",
            "value": " 7/7 [00:08&lt;00:00,  1.10s/ba]"
          }
        },
        "dbd93cf11f4643c89ca14ace4bf93498": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc2d92ec876e45bfaac882031e580161": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd9cbf26f7bf484795f8193a62098c69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e156e839e650459ca209b985b1a01914": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_34d6c317a04d4139a3dcae157dae828f",
            "placeholder": "​",
            "style": "IPY_MODEL_5a7785a920f14b2a84f3e8e917f02c12",
            "value": ""
          }
        },
        "e1f79ff76dcc4bcfa92b2347b1df0af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3c01649f0744b2a8b3b766edb911c26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e51590d7574b438d80904e6c36661b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bc573fb42724d7c969d3c7cff6f8858",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfae6bd55bf74126a0df0ec5f499060f",
            "value": 7
          }
        },
        "e6201e5aee0c4414bad27ea32ae39a92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6943ed1569b46f498af50c7cf195f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f7a6a4d75df4d08af50bac590c84a59",
            "placeholder": "​",
            "style": "IPY_MODEL_b7f6c5d7bc7f4df8a40ce983dd7c65e5",
            "value": "Downloading data files: 100%"
          }
        },
        "eb83a716f28a469b89cb85011327ef9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1aff7ea074948a1a5256c33bfb632ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f22cac22604f441ca818c3c3aa42aa42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2d8b839e1cb4b799b03d46277c7b446": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3024bfb689240828653eba11bbb70d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3aa20e336fb4a5290dc6e424c1fdbb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b731387ad93c499691167c4b77ae2811",
            "placeholder": "​",
            "style": "IPY_MODEL_4340593483c14282841abd25fee8f86a",
            "value": "Downloading data: 100%"
          }
        },
        "f61338d982704978bea80ff3fe2aadd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f702d59ce3af429fbb464b9d768a1d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8af2e05a1654404b7f8648cb37f0ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b56e454602941dda8fc6622eac051b9",
              "IPY_MODEL_76a025fdfd5e44ec8c73ca19f1d2f47d",
              "IPY_MODEL_98a00fe3243a4a5ab572634ddec68a38"
            ],
            "layout": "IPY_MODEL_5360d869ee2342cbaad6d4df3f7dc576"
          }
        },
        "f8bd2e7d752f47c483ebc2470cb6b9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9b1270cf7cf4ce0878261f3f1e289c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1c31f8d3b6463fb75f25ecf7879858",
            "placeholder": "​",
            "style": "IPY_MODEL_fc5528ed77f144b1bc69e2fbc608922b",
            "value": "100%"
          }
        },
        "fc5528ed77f144b1bc69e2fbc608922b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd1c31f8d3b6463fb75f25ecf7879858": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}